#!/usr/bin/env python3
"""
å‹åŠ›æµ‹è¯•è„šæœ¬

æµ‹è¯•ç³»ç»Ÿåœ¨é«˜è´Ÿè½½å’Œæç«¯æ¡ä»¶ä¸‹çš„è¡¨ç°
"""

import sys
import time
import random
import threading
import multiprocessing as mp
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import psutil

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

from quant_system.utils.performance import (
    performance_monitor, start_performance_monitoring, 
    stop_performance_monitoring, performance_context
)
from quant_system.utils.cache import cache_manager, LRUCache
from quant_system.utils.concurrent import parallel_map, WorkerPool

class StressTestRunner:
    """å‹åŠ›æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.results = {}
        self.errors = []
        
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰å‹åŠ›æµ‹è¯•"""
        print("ğŸ”¥ é‡åŒ–æŠ•èµ„ç³»ç»Ÿå‹åŠ›æµ‹è¯•")
        print("=" * 50)
        
        # å¯åŠ¨æ€§èƒ½ç›‘æ§
        start_performance_monitoring(interval=0.5)
        
        try:
            test_methods = [
                ("å¤§æ•°æ®é‡å¤„ç†", self.test_large_dataset_processing),
                ("é«˜å¹¶å‘è®¿é—®", self.test_high_concurrency),
                ("å†…å­˜å‹åŠ›", self.test_memory_stress),
                ("ç¼“å­˜å‹åŠ›", self.test_cache_stress),
                ("CPUå¯†é›†å‹ä»»åŠ¡", self.test_cpu_intensive),
                ("I/Oå¯†é›†å‹ä»»åŠ¡", self.test_io_intensive),
                ("é•¿æ—¶é—´è¿è¡Œ", self.test_long_running),
                ("å¼‚å¸¸æ¢å¤", self.test_exception_recovery)
            ]
            
            for test_name, test_method in test_methods:
                print(f"\nğŸ§ª {test_name}æµ‹è¯•...")
                try:
                    result = test_method()
                    self.results[test_name] = result
                    print(f"  âœ… {test_name}æµ‹è¯•é€šè¿‡")
                except Exception as e:
                    self.errors.append((test_name, str(e)))
                    print(f"  âŒ {test_name}æµ‹è¯•å¤±è´¥: {e}")
            
            # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
            self.generate_report()
            
        finally:
            stop_performance_monitoring()
    
    def generate_mock_stock_data(self, count: int):
        """ç”Ÿæˆæ¨¡æ‹Ÿè‚¡ç¥¨æ•°æ®"""
        stocks = []
        for i in range(count):
            stock = {
                'code': f"{random.randint(0, 999):03d}{random.randint(0, 999):03d}",
                'name': f"è‚¡ç¥¨{i+1}",
                'price': round(random.uniform(5, 100), 2),
                'volume': random.randint(100000, 10000000),
                'pct_change': round(random.uniform(-0.1, 0.1), 4),
                'market_cap': random.randint(1000000000, 100000000000),
                'pe_ratio': round(random.uniform(5, 50), 2),
                'pb_ratio': round(random.uniform(0.5, 5), 2)
            }
            stocks.append(stock)
        return stocks
    
    def test_large_dataset_processing(self):
        """æµ‹è¯•å¤§æ•°æ®é‡å¤„ç†"""
        test_sizes = [10000, 50000, 100000]
        results = {}
        
        for size in test_sizes:
            print(f"    æµ‹è¯•æ•°æ®é‡: {size}")
            
            # ç”Ÿæˆæ•°æ®
            start_time = time.time()
            data = self.generate_mock_stock_data(size)
            generation_time = time.time() - start_time
            
            # æ•°æ®å¤„ç†
            start_time = time.time()
            processed_data = []
            for stock in data:
                if (stock['price'] > 10 and 
                    stock['volume'] > 1000000 and 
                    stock['pct_change'] > 0):
                    processed_data.append(stock)
            processing_time = time.time() - start_time
            
            # æ’åº
            start_time = time.time()
            sorted_data = sorted(processed_data, key=lambda x: x['pct_change'], reverse=True)
            sorting_time = time.time() - start_time
            
            results[size] = {
                'generation_time': generation_time,
                'processing_time': processing_time,
                'sorting_time': sorting_time,
                'processed_count': len(processed_data),
                'total_time': generation_time + processing_time + sorting_time
            }
            
            print(f"      ç”Ÿæˆ: {generation_time:.3f}s, å¤„ç†: {processing_time:.3f}s, æ’åº: {sorting_time:.3f}s")
            print(f"      ç»“æœ: {len(data)} -> {len(processed_data)} -> {len(sorted_data)}")
        
        return results
    
    def test_high_concurrency(self):
        """æµ‹è¯•é«˜å¹¶å‘è®¿é—®"""
        def worker_task(worker_id):
            """å·¥ä½œçº¿ç¨‹ä»»åŠ¡"""
            results = []
            errors = []
            
            try:
                # æ¨¡æ‹Ÿæ•°æ®å¤„ç†
                data = self.generate_mock_stock_data(100)
                
                # æ¨¡æ‹Ÿè®¡ç®—
                for stock in data:
                    score = stock['price'] * stock['volume'] / 1000000
                    results.append((stock['code'], score))
                
                # æ’åº
                results.sort(key=lambda x: x[1], reverse=True)
                
                return {
                    'worker_id': worker_id,
                    'processed_count': len(results),
                    'top_stock': results[0] if results else None,
                    'success': True
                }
                
            except Exception as e:
                errors.append(str(e))
                return {
                    'worker_id': worker_id,
                    'error': str(e),
                    'success': False
                }
        
        # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
        concurrency_levels = [10, 50, 100]
        results = {}
        
        for level in concurrency_levels:
            print(f"    å¹¶å‘çº§åˆ«: {level}")
            
            start_time = time.time()
            
            with ThreadPoolExecutor(max_workers=level) as executor:
                futures = [executor.submit(worker_task, i) for i in range(level)]
                worker_results = [future.result() for future in futures]
            
            execution_time = time.time() - start_time
            
            successful_workers = [r for r in worker_results if r['success']]
            failed_workers = [r for r in worker_results if not r['success']]
            
            results[level] = {
                'execution_time': execution_time,
                'successful_workers': len(successful_workers),
                'failed_workers': len(failed_workers),
                'success_rate': len(successful_workers) / level,
                'throughput': level / execution_time
            }
            
            print(f"      æ‰§è¡Œæ—¶é—´: {execution_time:.3f}s")
            print(f"      æˆåŠŸç‡: {len(successful_workers)}/{level} ({results[level]['success_rate']:.2%})")
            print(f"      ååé‡: {results[level]['throughput']:.1f} ä»»åŠ¡/ç§’")
        
        return results
    
    def test_memory_stress(self):
        """æµ‹è¯•å†…å­˜å‹åŠ›"""
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        print(f"    åˆå§‹å†…å­˜: {initial_memory:.1f}MB")
        
        # é€æ­¥å¢åŠ å†…å­˜ä½¿ç”¨
        memory_data = []
        batch_sizes = [1000, 5000, 10000, 20000]
        
        results = {
            'initial_memory': initial_memory,
            'batches': []
        }
        
        for batch_size in batch_sizes:
            print(f"    åˆ›å»ºæ‰¹æ¬¡: {batch_size} ä¸ªå¯¹è±¡")
            
            # åˆ›å»ºå¤§é‡å¯¹è±¡
            batch_data = []
            for i in range(batch_size):
                obj = {
                    'id': i,
                    'data': f"test_data_{i}" * 100,  # æ¯ä¸ªå¯¹è±¡çº¦1KB
                    'numbers': list(range(100)),
                    'timestamp': time.time()
                }
                batch_data.append(obj)
            
            memory_data.append(batch_data)
            
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨
            current_memory = process.memory_info().rss / 1024 / 1024
            memory_growth = current_memory - initial_memory
            
            batch_result = {
                'batch_size': batch_size,
                'current_memory': current_memory,
                'memory_growth': memory_growth,
                'objects_count': sum(len(batch) for batch in memory_data)
            }
            
            results['batches'].append(batch_result)
            
            print(f"      å½“å‰å†…å­˜: {current_memory:.1f}MB (+{memory_growth:.1f}MB)")
            print(f"      å¯¹è±¡æ€»æ•°: {batch_result['objects_count']}")
        
        # æ¸…ç†å†…å­˜
        del memory_data
        import gc
        gc.collect()
        
        final_memory = process.memory_info().rss / 1024 / 1024
        results['final_memory'] = final_memory
        results['memory_released'] = results['batches'][-1]['current_memory'] - final_memory
        
        print(f"    æ¸…ç†åå†…å­˜: {final_memory:.1f}MB")
        print(f"    é‡Šæ”¾å†…å­˜: {results['memory_released']:.1f}MB")
        
        return results
    
    def test_cache_stress(self):
        """æµ‹è¯•ç¼“å­˜å‹åŠ›"""
        cache_sizes = [100, 1000, 10000]
        results = {}
        
        for cache_size in cache_sizes:
            print(f"    ç¼“å­˜å¤§å°: {cache_size}")
            
            cache = LRUCache(max_size=cache_size, ttl=60)
            
            # å†™å…¥æµ‹è¯•
            start_time = time.time()
            for i in range(cache_size * 2):  # å†™å…¥2å€å®¹é‡çš„æ•°æ®
                key = f"key_{i}"
                value = f"value_{i}" * 100  # æ¯ä¸ªå€¼çº¦600å­—èŠ‚
                cache.put(key, value)
            write_time = time.time() - start_time
            
            # è¯»å–æµ‹è¯•
            start_time = time.time()
            hit_count = 0
            miss_count = 0
            
            for i in range(cache_size * 2):
                key = f"key_{i}"
                value = cache.get(key)
                if value is not None:
                    hit_count += 1
                else:
                    miss_count += 1
            
            read_time = time.time() - start_time
            
            cache_stats = cache.stats()
            
            results[cache_size] = {
                'write_time': write_time,
                'read_time': read_time,
                'hit_count': hit_count,
                'miss_count': miss_count,
                'hit_rate': hit_count / (hit_count + miss_count),
                'cache_stats': cache_stats
            }
            
            print(f"      å†™å…¥æ—¶é—´: {write_time:.3f}s")
            print(f"      è¯»å–æ—¶é—´: {read_time:.3f}s")
            print(f"      å‘½ä¸­ç‡: {results[cache_size]['hit_rate']:.2%}")
        
        return results
    
    def test_cpu_intensive(self):
        """æµ‹è¯•CPUå¯†é›†å‹ä»»åŠ¡"""
        def cpu_task(n):
            """CPUå¯†é›†å‹ä»»åŠ¡"""
            result = 0
            for i in range(n * 10000):
                result += i ** 2
            return result
        
        task_sizes = [100, 500, 1000]
        results = {}
        
        for size in task_sizes:
            print(f"    ä»»åŠ¡è§„æ¨¡: {size}")
            
            tasks = [size] * 10  # 10ä¸ªç›¸åŒè§„æ¨¡çš„ä»»åŠ¡
            
            # ä¸²è¡Œæ‰§è¡Œ
            start_time = time.time()
            serial_results = [cpu_task(task) for task in tasks]
            serial_time = time.time() - start_time
            
            # å¹¶è¡Œæ‰§è¡Œï¼ˆçº¿ç¨‹ï¼‰
            start_time = time.time()
            thread_results = parallel_map(cpu_task, tasks, use_processes=False, max_workers=4)
            thread_time = time.time() - start_time
            
            # å¹¶è¡Œæ‰§è¡Œï¼ˆè¿›ç¨‹ï¼‰
            start_time = time.time()
            process_results = parallel_map(cpu_task, tasks, use_processes=True, max_workers=4)
            process_time = time.time() - start_time
            
            # éªŒè¯ç»“æœä¸€è‡´æ€§
            assert serial_results == thread_results == process_results
            
            results[size] = {
                'serial_time': serial_time,
                'thread_time': thread_time,
                'process_time': process_time,
                'thread_speedup': serial_time / thread_time,
                'process_speedup': serial_time / process_time
            }
            
            print(f"      ä¸²è¡Œ: {serial_time:.3f}s")
            print(f"      çº¿ç¨‹: {thread_time:.3f}s (åŠ é€Ÿ {results[size]['thread_speedup']:.2f}x)")
            print(f"      è¿›ç¨‹: {process_time:.3f}s (åŠ é€Ÿ {results[size]['process_speedup']:.2f}x)")
        
        return results
    
    def test_io_intensive(self):
        """æµ‹è¯•I/Oå¯†é›†å‹ä»»åŠ¡"""
        def io_task(delay):
            """I/Oå¯†é›†å‹ä»»åŠ¡ï¼ˆæ¨¡æ‹Ÿï¼‰"""
            time.sleep(delay)
            return f"Task completed after {delay}s"
        
        delays = [0.1] * 20  # 20ä¸ª0.1ç§’çš„ä»»åŠ¡
        
        # ä¸²è¡Œæ‰§è¡Œ
        start_time = time.time()
        serial_results = [io_task(delay) for delay in delays]
        serial_time = time.time() - start_time
        
        # å¹¶è¡Œæ‰§è¡Œ
        start_time = time.time()
        parallel_results = parallel_map(io_task, delays, use_processes=False, max_workers=10)
        parallel_time = time.time() - start_time
        
        results = {
            'task_count': len(delays),
            'serial_time': serial_time,
            'parallel_time': parallel_time,
            'speedup': serial_time / parallel_time,
            'efficiency': (serial_time / parallel_time) / 10  # 10ä¸ªå·¥ä½œçº¿ç¨‹
        }
        
        print(f"    ä»»åŠ¡æ•°é‡: {results['task_count']}")
        print(f"    ä¸²è¡Œæ—¶é—´: {serial_time:.3f}s")
        print(f"    å¹¶è¡Œæ—¶é—´: {parallel_time:.3f}s")
        print(f"    åŠ é€Ÿæ¯”: {results['speedup']:.2f}x")
        print(f"    æ•ˆç‡: {results['efficiency']:.2%}")
        
        return results
    
    def test_long_running(self):
        """æµ‹è¯•é•¿æ—¶é—´è¿è¡Œ"""
        print("    è¿è¡Œ30ç§’é•¿æ—¶é—´æµ‹è¯•...")
        
        start_time = time.time()
        end_time = start_time + 30  # è¿è¡Œ30ç§’
        
        iteration_count = 0
        memory_samples = []
        
        process = psutil.Process()
        
        while time.time() < end_time:
            # æ¨¡æ‹Ÿå·¥ä½œè´Ÿè½½
            data = self.generate_mock_stock_data(100)
            processed = [s for s in data if s['price'] > 20]
            sorted_data = sorted(processed, key=lambda x: x['pct_change'])
            
            iteration_count += 1
            
            # æ¯5ç§’é‡‡æ ·ä¸€æ¬¡å†…å­˜
            if iteration_count % 100 == 0:
                memory_mb = process.memory_info().rss / 1024 / 1024
                memory_samples.append(memory_mb)
                elapsed = time.time() - start_time
                print(f"      {elapsed:.1f}s: è¿­ä»£ {iteration_count}, å†…å­˜ {memory_mb:.1f}MB")
        
        total_time = time.time() - start_time
        
        results = {
            'total_time': total_time,
            'iteration_count': iteration_count,
            'iterations_per_second': iteration_count / total_time,
            'memory_samples': memory_samples,
            'memory_stable': max(memory_samples) - min(memory_samples) < 50 if memory_samples else True
        }
        
        print(f"    æ€»æ—¶é—´: {total_time:.1f}s")
        print(f"    æ€»è¿­ä»£: {iteration_count}")
        print(f"    è¿­ä»£é€Ÿç‡: {results['iterations_per_second']:.1f} æ¬¡/ç§’")
        print(f"    å†…å­˜ç¨³å®š: {'æ˜¯' if results['memory_stable'] else 'å¦'}")
        
        return results
    
    def test_exception_recovery(self):
        """æµ‹è¯•å¼‚å¸¸æ¢å¤"""
        def failing_task(should_fail):
            """å¯èƒ½å¤±è´¥çš„ä»»åŠ¡"""
            if should_fail:
                raise ValueError("æ¨¡æ‹Ÿä»»åŠ¡å¤±è´¥")
            return "ä»»åŠ¡æˆåŠŸ"
        
        # æµ‹è¯•å¼‚å¸¸å¤„ç†
        tasks = [False] * 5 + [True] * 3 + [False] * 2  # æ··åˆæˆåŠŸå’Œå¤±è´¥çš„ä»»åŠ¡
        
        success_count = 0
        error_count = 0
        results = []
        
        for i, should_fail in enumerate(tasks):
            try:
                result = failing_task(should_fail)
                results.append(('success', result))
                success_count += 1
            except Exception as e:
                results.append(('error', str(e)))
                error_count += 1
        
        # æµ‹è¯•ç³»ç»Ÿåœ¨å¼‚å¸¸åçš„æ¢å¤èƒ½åŠ›
        recovery_test_passed = True
        try:
            # åœ¨å¼‚å¸¸åç»§ç»­æ­£å¸¸å·¥ä½œ
            normal_data = self.generate_mock_stock_data(10)
            assert len(normal_data) == 10
        except Exception:
            recovery_test_passed = False
        
        test_results = {
            'total_tasks': len(tasks),
            'success_count': success_count,
            'error_count': error_count,
            'success_rate': success_count / len(tasks),
            'recovery_test_passed': recovery_test_passed
        }
        
        print(f"    æ€»ä»»åŠ¡: {test_results['total_tasks']}")
        print(f"    æˆåŠŸ: {success_count}, å¤±è´¥: {error_count}")
        print(f"    æˆåŠŸç‡: {test_results['success_rate']:.2%}")
        print(f"    æ¢å¤æµ‹è¯•: {'é€šè¿‡' if recovery_test_passed else 'å¤±è´¥'}")
        
        return test_results
    
    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 50)
        print("ğŸ“Š å‹åŠ›æµ‹è¯•æŠ¥å‘Š")
        print("=" * 50)
        
        if self.errors:
            print(f"\nâŒ å¤±è´¥çš„æµ‹è¯• ({len(self.errors)}):")
            for test_name, error in self.errors:
                print(f"  - {test_name}: {error}")
        
        if self.results:
            print(f"\nâœ… æˆåŠŸçš„æµ‹è¯• ({len(self.results)}):")
            for test_name in self.results:
                print(f"  - {test_name}")
        
        # ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
        print(f"\nğŸ’» ç³»ç»Ÿèµ„æº:")
        print(f"  CPUæ ¸å¿ƒæ•°: {mp.cpu_count()}")
        print(f"  å†…å­˜æ€»é‡: {psutil.virtual_memory().total / 1024 / 1024 / 1024:.1f}GB")
        print(f"  å½“å‰å†…å­˜ä½¿ç”¨: {psutil.virtual_memory().percent:.1f}%")
        
        # æ€§èƒ½å»ºè®®
        print(f"\nğŸ’¡ æ€§èƒ½å»ºè®®:")
        if len(self.errors) == 0:
            print("  âœ… ç³»ç»Ÿåœ¨å‹åŠ›æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½")
            print("  âœ… æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†å‹åŠ›æµ‹è¯•")
        else:
            print("  âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œå»ºè®®æ£€æŸ¥ç›¸å…³ç»„ä»¶")
        
        print("  ğŸ“ˆ å»ºè®®å®šæœŸè¿›è¡Œå‹åŠ›æµ‹è¯•ä»¥ç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§")
        print("  ğŸ”§ æ ¹æ®å®é™…è´Ÿè½½è°ƒæ•´ç³»ç»Ÿé…ç½®å‚æ•°")

def main():
    """ä¸»å‡½æ•°"""
    runner = StressTestRunner()
    runner.run_all_tests()

if __name__ == "__main__":
    main()
